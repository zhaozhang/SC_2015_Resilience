We thank the reviewers for their thoughtful comments, and provide additional information relating to their points, reorganized the rebuttal questions as follows.

1) Applicability to supercomputers.
The current barriers that prevents a FUSE-mounted file system running on supercomputers are both technical and policy. On Cray supercomputers, the Cluster Compatibility Mode allows users to access FUSE, but this support is typically disabled by administrators. The previous generation BG/P system supports FUSE via the ZeptoOS kernel; this is the basis for AMFORA. In previous work we have demonstrated that AMFORA scales to 8K nodes on BG/P. The successor of BG/P, BG/Q, does not have a full Linux kernel and therefore does not support FUSE. However, researchers working on the Argo operating system (http://www.mcs.anl.gov/project/argo-exascale-operating-system) will solve this issue. We therefore strongly believe that our approach is applicable to supercomputers. 
In this work we emulate a supercomputer by using EC2 instances. This approach is representative, as we only make use of the CPU, memory and network, which emulates a supercomputer that doesn't have local disk storage. Clearly, network bandwidth is lower in EC2 than that of a supercomputer, but since our files are moderate in size, network transfer is more latency-bound than bandwidth-bound. Importantly, our model is designed to be tuned to network conditions. On a supercomputer, we can specify higher bandwidth and lower latency parameters. Thus, we believe our model is equally applicable on a supercomputer.

2) What if asynchronous backup is available?
Intuitively, asynchronous backup should run faster than synchronous backup. However, previous work [1] has shown that simple asynchronous backup can perform worse than synchronous backup. The asynchronous approach also requires additional logic to guarantee consistency. If failure happens during asynchronous backup, it is extremely difficult to determine if lost data can be recovered from replicas. From another perspective, our adaptive method only introduces 2.9% overhead for the Montage application. In other cases where replication is the chosen backup method, there is a relatively longer execution time to produce that data. The relatively longer execution time will make the replication overhead less significant if not negligible.

3) Application(s) were not convincing.
The Montage application contains nine stages, which can be viewed as nine independent? applications. Figure 5 in the paper shows that these nine stages cover all possible combinations in the replication-reexecution decision space. We believe that using the nine stages of Montage demonstrates the effectiveness of our adaptive method across a broad range of applications.

4) What is the novelty of this approach?
The novelty of this work is the dynamic backup decision making at runtime. Most existing systems use monolithic approaches throughout application stages. Monolithic lineage makes deeper stages too expensive to be recovered, while uniform replication slows down overall application performance. Our approach is able to achieve the best of both approaches. 

[1] Sato et al., "Design and Modeling of a Non-blocking Checkpointing System", SC'12, http://dl.acm.org/citation.cfm?id=2389022
