We thank the reviewers for their thoughtful comments, and provide additional information relating to their points, reorganized as follows.

1) Applicability to supercomputers.
The current barriers that prevents a FUSE-mounted file system running on supercomputers are both technical and policy. On Cray supercomputers, the Cluster Compatibility Mode allows users to use FUSE, but this support is typically disabled by administrators. BG/P systems support FUSE via the ZeptoOS kernel; this is the basis for AMFORA, which we previously demonstrated scaling to 8K nodes on BG/P. The BG/P successor, BG/Q, does not have a full Linux kernel and therefore does not support FUSE. However, researchers working on the Argo operating system (http://www.mcs.anl.gov/project/argo-exascale-operating-system) will solve this issue. We therefore strongly believe that our approach is applicable to supercomputers. 
In this work we emulate a supercomputer by using EC2 instances. This approach is representative, as we only make use of the CPU, memory, and network, which emulate a supercomputer that doesn't have local disk storage. Clearly, EC2 network bandwidth is lower than a supercomputer's, but since our files are moderately sized, network transfer is more latency-bound than bandwidth-bound. Importantly, our model is designed to be tuned to network conditions. On a supercomputer, we can specify higher bandwidth and lower latency parameters. Thus, we believe our model is equally applicable on a supercomputer.

2) What if asynchronous backup is available?
Intuitively, asynchronous backup should run faster than synchronous backup. However, previous work [1] has shown that simple asynchronous backup can perform worse than synchronous backup. The asynchronous approach also requires additional logic to guarantee consistency. If failure happens during asynchronous backup, it is extremely difficult to determine if lost data can be recovered from replicas. From another perspective, our adaptive method only introduces 2.9% overhead for the Montage application. In other cases where replication is the chosen backup method, there is a relatively longer execution time to produce that data. The relatively longer execution time will make the replication overhead less significant if not negligible.

3) Application(s) were not convincing.
The Montage application contains nine stages, which can be viewed as nine distinct applications. Figure 5 in the paper shows that these nine stages cover all possible combinations of the replication-reexecution decision space. We believe that using the nine stages of Montage demonstrates the effectiveness of our adaptive method across a broad range of applications.

4) What is the novelty of this approach?
The novelty of this work is the dynamic backup decisions made at runtime. Most existing systems use monolithic approaches throughout application stages. Monolithic lineage makes deeper stages too expensive to be recovered, while uniform replication slows down overall application performance. Our approach is able to achieve the best of both approaches. 

[1] Sato et al., "Design and Modeling of a Non-blocking Checkpointing System", SC'12, http://dl.acm.org/citation.cfm?id=2389022
