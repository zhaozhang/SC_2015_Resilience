We thank the reviewers for their thoughtful comments. We provide here additional information relating to the points that they made. We reorganized the rebuttal questions as following and addressed them.

1) Applicability to supercomputers.
The current barrier that prevents a fuse-mounted file system running on supercomputers is both a policy and technology issue. On Cray supercomputers, the Cluster Compatibility Mode does allow users to access fuse library, however this support is typically disabled by administrators. The previous generation BG/P system supports fuse via the ZeptoOS kernel--and this is the basis for AMFORA. In previous work we have demonstrated that AMFORA scales to 8K nodes. The successor of BG/P, the BG/Q system, does not have a full Linux kernel and therefore does not support fuse, however researchers are working on the Argo operating system (http://www.mcs.anl.gov/project/argo-exascale-operating-system) to solve this issue. We therefore strongly believe that our approach is applicable to supercomputers. 
The way we emulate a supercomputer by using the EC2 instances is representative as we only make use of the CPU, memory and network. This emulates a supercomputer without local disk storage. Though the network bandwidth is lower in EC2 than that of supercomputer, since the files that are being replicated are relatively small. The network transfer is more latency-bounded rather than bandwidth-bounded. On a supercomputer, we can specify a higher bandwidth and lower latency parameter to adapt this adaptive method working as effective on a supercomputer as it does on EC2.

[KC - should we say something here about our results being in some way representative for supercomputers?]
[ZZ - add something]

2) What if asynchronous backup is available?
Intuitively, asynchronous backup should run faster than synchronous backup. However, previous work [1] has shown that simple asynchronous backup can be worse than synchronous backup. The asynchronous approach also requires additional logic to guarantee consistency. If failure happens duringasynchronous backup, it is extremely difficult to deterimine if lost data can be recovered from replicas. From another perspective, our adaptive method only introduces 2.9% overhead
for the Montage application. In other cases where replicating data is the backup choice, there is a relatively longer execution time to produce that data. The relatively longer running time will make the replication overhead less significant if not negligible. 


3) Application(s) were not convincing.
The Montage application contains nine stages, which can be viewed as nine identical [KC - do you mean independent?] applications. Figure 5 in the paper shows that these nine stages cover all possible combinations in the replication-reexecution decision space. We believe that using the nine stages of Montage demonstrates the effectiveness of our adaptive method.


4) What is the novelty of this approach?
The novelty of this work is the dynamic backup decision making at runtime. 
Most existing systems use monolithic approaches throughout the stages, where monolithic lineage will make deeper stages too expensive to be recovered, while uniform replication slows down overall application performance.

[1] Sato et al., "Design and Modeling of a Non-blocking Checkpointing System", SC'12, http://dl.acm.org/citation.cfm?id=2389022