We thank the reviewers for their thoughtful comments. We provide here additional information relating to the points that they made. We reorganized the rebuttal questions as following and addressed them.

1) Applicability to supercomputers.
The current barrier that prevents a fuse-mounted file system running on supercomputers is both a policy and technology issue. On Cray supercomputers, the Cluster Compatibility Mode does allow users to access fuse, however this support is typically disabled by administrators. The previous generation BG/P system supports fuse via the ZeptoOS kernel--this is the basis for AMFORA. In previous work we have demonstrated that AMFORA scales to 8K nodes on BG/P. The successor of BG/P, the BG/Q system, does not have a full Linux kernel and therefore does not support fuse, however researchers are working on the Argo operating system (http://www.mcs.anl.gov/project/argo-exascale-operating-system) to solve this issue. We therefore strongly believe that our approach is applicable to supercomputers. 
In this work we emulate a supercomputer by using EC2 instances. This approach is representative as we only make use of the CPU, memory and network. This emulates a supercomputer without local disk storage. Clearly, network bandwidth is lower in EC2 than that of a supercomputer. However, as the files are moderate in size, network transfer is more latency-bounded than bandwidth-bounded. Importantly, our model is designed with to be tuned to network conditions. On a supercomputer, we can specify a higher bandwidth and lower latency parameter. Thus, we believe our model is equally applicable on a supercomputer.

2) What if asynchronous backup is available?
Intuitively, asynchronous backup should run faster than synchronous backup. However, previous work [1] has shown that simple asynchronous backup can perform worse than synchronous backup. The asynchronous approach also requires additional logic to guarantee consistency. If failure happens during asynchronous backup, it is extremely difficult to determine if lost data can be recovered from replicas. From another perspective, our adaptive method only introduces 2.9% overhead
for the Montage application. In other cases where replicating data is the chosen backup method, there is a relatively longer execution time to produce that data. The relatively longer execution time will make the replication overhead less significant if not negligible. 


3) Application(s) were not convincing.
The Montage application contains nine stages, which can be viewed as nine identical [KC - do you mean independent?] applications. Figure 5 in the paper shows that these nine stages cover all possible combinations in the replication-reexecution decision space. We believe that using the nine stages of Montage demonstrates the effectiveness of our adaptive method across a broad range of applications.


4) What is the novelty of this approach?
The novelty of this work is the dynamic backup decision making at runtime. 
Most existing systems use monolithic approaches throughout application stages, where monolithic lineage will make deeper stages too expensive to be recovered, while uniform replication slows down overall application performance. 
Our approach is able to achieve the best of both approaches with only minimal overhead. 

[1] Sato et al., "Design and Modeling of a Non-blocking Checkpointing System", SC'12, http://dl.acm.org/citation.cfm?id=2389022